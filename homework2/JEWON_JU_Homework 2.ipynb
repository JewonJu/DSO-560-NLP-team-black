{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9896b62",
   "metadata": {},
   "source": [
    "# Homework 2 (Due Thursday Dec 1, 6:29pm PST)\n",
    "\n",
    "Please submit as a notebook in the format `HW2_FIRSTNAME_LASTNAME_USCID.ipynb` in a group chat to me and the TAs.\n",
    "\n",
    "Your `USCID` is your student 10-digit ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be029ae1",
   "metadata": {},
   "source": [
    "### Part I.  Topic Modelling and Analysis (5pts)\n",
    "\n",
    "Pick from **one** of the dataset options below:\n",
    "* **Negative McDonalds Yelp reviews**: `datasets/mcdonalds-yelp-negative-reviews.csv`\n",
    "* **[Top 5000 Udemy courses](https://www.kaggle.com/datasets/90eededa5561eee7f62c0e68ecdad14c2bdb58bc923834067025dee655a6083e?resource=download)** - a Kaggle dataset of the course descriptions of the top 5000 Udemy courses in 2022: `datasets/top5000_udemy.csv`\n",
    "\n",
    "In your notebook, explore the data and perform topic modelling. You may use any vectorization or text preprocessing techniques we have discussed.\n",
    "\n",
    "In order to earn full credit, you must:\n",
    "\n",
    "* Show the **# of topics you tried, and explain why you ultimately decided on the final #**.\n",
    "* Demonstrate **adequate text preprocessing (there are likely obvious stopwords / fuzzy matching / regex groupings that can be done to improve the final results)** - show what you tried.\n",
    "* In 2-3 sentences: A **business analysis of these topics - what do they reveal as actionable next steps or insights for McDonalds or Udemy?** Please be specific in your recommendations/insights.\n",
    "    - **Not specific**: *We recommend Amazon look into the quality of their toys, since the reviews show disatisfaction with the value of their product.*\n",
    "    - **Specific**: *Amazon should explore more durable batteries/hardwares. For example, X% of reviews mention that the toys' batteries were broken or immediately died. This is part of a larger theme of components not being ready to use out the box, which often leads to disappointment on holiday occasions when children open up their gifts. See the following document snippets as examples:...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1167b72",
   "metadata": {},
   "source": [
    "#### 1. Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af093c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "reviews = pd.read_csv(\"../datasets/mcdonalds-yelp-negative-reviews.csv\", encoding='latin-1')\n",
    "text = reviews[\"review\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eea44a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>city</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679455653</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I'm not a huge mcds lover, but I've been to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679455654</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Terrible customer service. I came in at 9:30pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>679455655</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>First they \"lost\" my order, actually they gave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>679455656</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I see I'm not the only one giving 1 star. Only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679455657</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Well, it's McDonald's, so you know what the fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id     city                                             review\n",
       "0  679455653  Atlanta  I'm not a huge mcds lover, but I've been to be...\n",
       "1  679455654  Atlanta  Terrible customer service. I came in at 9:30pm...\n",
       "2  679455655  Atlanta  First they \"lost\" my order, actually they gave...\n",
       "3  679455656  Atlanta  I see I'm not the only one giving 1 star. Only...\n",
       "4  679455657  Atlanta  Well, it's McDonald's, so you know what the fo..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d7e85",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning and Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4244b5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>city</th>\n",
       "      <th>review</th>\n",
       "      <th>reviews_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679455653</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I'm not a huge mcds lover, but I've been to be...</td>\n",
       "      <td>i m not a huge mcds lover  but i ve been to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679455654</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Terrible customer service. I came in at 9:30pm...</td>\n",
       "      <td>terrible customer service  i came in at 9 30pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>679455655</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>First they \"lost\" my order, actually they gave...</td>\n",
       "      <td>first they  lost  my order  actually they gave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>679455656</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I see I'm not the only one giving 1 star. Only...</td>\n",
       "      <td>i see i m not the only one giving 1 star  only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679455657</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Well, it's McDonald's, so you know what the fo...</td>\n",
       "      <td>well  it s mcdonald s  so you know what the fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>679455658</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>This has to be one of the worst and slowest Mc...</td>\n",
       "      <td>this has to be one of the worst and slowest mc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>679455659</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I'm not crazy about this McDonald's. This is p...</td>\n",
       "      <td>i m not crazy about this mcdonald s  this is p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>679455660</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>One Star and I'm beng kind. I blame management...</td>\n",
       "      <td>one star and i m beng kind  i blame management...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>679455661</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Never been upset about any fast food drive thr...</td>\n",
       "      <td>never been upset about any fast food drive thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>679455662</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>This McDonald's has gotten much better. Usuall...</td>\n",
       "      <td>this mcdonald s has gotten much better  usuall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id     city                                             review                                  reviews_processed\n",
       "0  679455653  Atlanta  I'm not a huge mcds lover, but I've been to be...  i m not a huge mcds lover  but i ve been to be...\n",
       "1  679455654  Atlanta  Terrible customer service. I came in at 9:30pm...  terrible customer service  i came in at 9 30pm...\n",
       "2  679455655  Atlanta  First they \"lost\" my order, actually they gave...  first they  lost  my order  actually they gave...\n",
       "3  679455656  Atlanta  I see I'm not the only one giving 1 star. Only...  i see i m not the only one giving 1 star  only...\n",
       "4  679455657  Atlanta  Well, it's McDonald's, so you know what the fo...  well  it s mcdonald s  so you know what the fo...\n",
       "5  679455658  Atlanta  This has to be one of the worst and slowest Mc...  this has to be one of the worst and slowest mc...\n",
       "6  679455659  Atlanta  I'm not crazy about this McDonald's. This is p...  i m not crazy about this mcdonald s  this is p...\n",
       "7  679455660  Atlanta  One Star and I'm beng kind. I blame management...  one star and i m beng kind  i blame management...\n",
       "8  679455661  Atlanta  Never been upset about any fast food drive thr...  never been upset about any fast food drive thr...\n",
       "9  679455662  Atlanta  This McDonald's has gotten much better. Usuall...  this mcdonald s has gotten much better  usuall..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['reviews_processed'] = reviews['review']\n",
    "\n",
    "# Remove punctuation\n",
    "from textacy.preprocessing.remove import punctuation\n",
    "reviews['reviews_processed'] = reviews['reviews_processed'].apply(punctuation)\n",
    "\n",
    "# Convert to lowercase \n",
    "reviews['reviews_processed'] = reviews['reviews_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Replace common entities/concepts \n",
    "from textacy.preprocessing.replace import urls, hashtags, numbers, emails, emojis, currency_symbols\n",
    "#reviews['reviews_processed'] = reviews['reviews_processed'].\\\n",
    "#  apply(urls).\\\n",
    "#  apply(hashtags).\\\n",
    "#  apply(numbers).\\\n",
    "#  apply(currency_symbols).\\\n",
    "#  apply(emojis).\\\n",
    "#  apply(emails)\n",
    "\n",
    "# Remove or normalize undesired text elements \n",
    "from collections import Counter\n",
    "from textacy.preprocessing.normalize import quotation_marks, bullet_points\n",
    "quotes = ['\"','“','”']\n",
    "\n",
    "reviews.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3582d40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    m huge mcds lover ve better ones far worst ve ...\n",
       "1    terrible customer service came 9 30pm stood re...\n",
       "2    lost order actually gave took 20 minutes figur...\n",
       "3                       m giving 1 star 25 star s need\n",
       "4    s mcdonald s know food review reflects solely ...\n",
       "5    worst slowest mcdonald s franchises t figure e...\n",
       "6    m crazy mcdonald s primarily slow gosh exactly...\n",
       "7    star m beng kind blame management day free cof...\n",
       "8    upset fast food drive service till came mcdona...\n",
       "9    mcdonald s gotten better usually order wrong s...\n",
       "Name: reviews_processed, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stopwords using gensim \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "reviews['reviews_processed'] = reviews['reviews_processed'].apply(remove_stopwords)\n",
    "reviews['reviews_processed'].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c441c542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF: (1525, 168)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10 minutes food</th>\n",
       "      <th>10 minutes fries</th>\n",
       "      <th>10 minutes later</th>\n",
       "      <th>10 minutes order</th>\n",
       "      <th>10 piece chicken</th>\n",
       "      <th>15 minutes drive</th>\n",
       "      <th>15 minutes later</th>\n",
       "      <th>20 minutes drive</th>\n",
       "      <th>20 minutes order</th>\n",
       "      <th>24 hour drive</th>\n",
       "      <th>...</th>\n",
       "      <th>window pick food</th>\n",
       "      <th>wish negative stars</th>\n",
       "      <th>worst customer service</th>\n",
       "      <th>worst fast food</th>\n",
       "      <th>worst mcdonald ve</th>\n",
       "      <th>worst mcdonalds planet</th>\n",
       "      <th>worst mcdonalds ve</th>\n",
       "      <th>worst service ve</th>\n",
       "      <th>write review mcdonald</th>\n",
       "      <th>write review mcdonalds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10 minutes food  10 minutes fries  10 minutes later  10 minutes order  10 piece chicken  15 minutes drive  15 minutes later  20 minutes drive  20 minutes order  24 hour drive  ...  window pick food  wish negative stars  worst customer service  worst fast food  worst mcdonald ve  worst mcdonalds planet  worst mcdonalds ve  worst service ve  write review mcdonald  write review mcdonalds\n",
       "0              0.0               0.0               0.0               0.0               0.0               0.0               0.0               0.0               0.0            0.0  ...               0.0                  0.0                     0.0              0.0                0.0                     0.0                 0.0               0.0                    0.0                     0.0\n",
       "1              0.0               0.0               0.0               0.0               0.0               0.0               0.0               0.0               0.0            0.0  ...               0.0                  0.0                     0.0              0.0                0.0                     0.0                 0.0               0.0                    0.0                     0.0\n",
       "2              0.0               0.0               0.0               0.0               0.0               0.0               0.0               0.0               0.0            0.0  ...               0.0                  0.0                     0.0              0.0                0.0                     0.0                 0.0               0.0                    0.0                     0.0\n",
       "3              0.0               0.0               0.0               0.0               0.0               0.0               0.0               0.0               0.0            0.0  ...               0.0                  0.0                     0.0              0.0                0.0                     0.0                 0.0               0.0                    0.0                     0.0\n",
       "4              0.0               0.0               0.0               0.0               0.0               0.0               0.0               0.0               0.0            0.0  ...               0.0                  0.0                     0.0              0.0                0.0                     0.0                 0.0               0.0                    0.0                     0.0\n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the corpus \n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,3), min_df=3,\n",
    "                            max_df=0.4, stop_words=\"english\")\n",
    "\n",
    "X, terms = vectorizer.fit_transform(reviews['reviews_processed']), vectorizer.get_feature_names_out()\n",
    "tf_idf = pd.DataFrame(X.toarray(), columns=terms)\n",
    "\n",
    "print(f\"TF-IDF: {tf_idf.shape}\")\n",
    "tf_idf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3451fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of X is (1525, 168)\n",
      "Decomposed W matrix is (1525, 5)\n",
      "Decomposed H matrix is (5, 168)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jewonju/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF Model\n",
    "nmf = NMF(n_components=5)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "print(f\"Original shape of X is {X.shape}\")\n",
    "print(f\"Decomposed W matrix is {W.shape}\")\n",
    "print(f\"Decomposed H matrix is {H.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7fd4b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 0\n",
      "\n",
      "\bopen 24 hours (60.4%)\n",
      "\n",
      "\bdrive open 24 (11.7%)\n",
      "\n",
      "\bcustomer service place (3.4%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 1\n",
      "\n",
      "\bgot order wrong (50.7%)\n",
      "\n",
      "\border wrong time (6.4%)\n",
      "\n",
      "\bmcdonald ve ve (4.3%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 2\n",
      "\n",
      "\beat fast food (71.7%)\n",
      "\n",
      "\bfast food restaurants (8.1%)\n",
      "\n",
      "\bgood service quick (6.8%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 3\n",
      "\n",
      "\bbacon egg cheese (36.7%)\n",
      "\n",
      "\begg cheese biscuit (30.5%)\n",
      "\n",
      "\bfree wi fi (11.3%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 4\n",
      "\n",
      "\bworst customer service (44.4%)\n",
      "\n",
      "\bcustomer service ve (9.8%)\n",
      "\n",
      "\bdrive order wrong (5.6%)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "# Report Results \n",
    "def get_top_tf_idf_tokens_for_topic(H: np.array, feature_names: List[str], num_top_tokens: int = 5):\n",
    "  \"\"\"\n",
    "  Uses the H matrix (K components x M original features) to identify for each\n",
    "  topic the most frequent tokens.\n",
    "  \"\"\"\n",
    "  for topic, vector in enumerate(H):\n",
    "    print(f\"TOPIC {topic}\\n\")\n",
    "    total = vector.sum()\n",
    "    top_scores = vector.argsort()[::-1][:num_top_tokens]\n",
    "    token_names = list(map(lambda idx: feature_names[idx], top_scores))\n",
    "    strengths = list(map(lambda idx: vector[idx] / total, top_scores))\n",
    "    \n",
    "    for strength, token_name in zip(strengths, token_names):\n",
    "      print(f\"\\b{token_name} ({round(strength * 100, 1)}%)\\n\")\n",
    "    print(f\"=\" * 50)\n",
    "\n",
    "get_top_tf_idf_tokens_for_topic(H, tf_idf.columns.tolist(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36b905",
   "metadata": {},
   "source": [
    "### Part II. Emotion Classification (5 pts)\n",
    "\n",
    "Use the `datasets/emotions_dataset.zip` (see the original Dataset source on [Kaggle](https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp)) to build a classification model that predicts the emotion of sentence. If you would like, you may classify only the top 4 emotions, and group all other classes as `Other`. \n",
    "\n",
    "In order to earn full credit, you must:\n",
    "\n",
    "* Show the performance of your model with `CountVectorizer`, `TfIdfVectorizer`, `word2vec`, and `glove` embeddings.\n",
    "    - for `word2vec`, make sure not to use the `en_core_web_sm` dataset (these are not real embeddings)\n",
    "* Perform text preprocessing (or explain why it was not necessary):\n",
    "    - stopword removal\n",
    "    - ngram tokenization\n",
    "    - stemming/lemmatization\n",
    "    - fuzzy matching / regex cleaning / etc. (as you deem necessary, but show that you analyzed the text to make your decision)\n",
    "* Show **AUROC / F1 scores** for on the holdout (test + validation) datasets.\n",
    "* A brief discussion (2-3 sentences) of what could improve your model and why."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed343a87219b975df84d5260125e17249ec3c1727fa796fd58ecd90702b8aed1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
