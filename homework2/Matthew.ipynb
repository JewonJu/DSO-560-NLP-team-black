{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9896b62",
   "metadata": {},
   "source": [
    "# Homework 2 (Due Thursday Dec 1, 6:29pm PST)\n",
    "\n",
    "Please submit as a notebook in the format `HW2_FIRSTNAME_LASTNAME_USCID.ipynb` in a group chat to me and the TAs.\n",
    "\n",
    "Your `USCID` is your student 10-digit ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36b905",
   "metadata": {},
   "source": [
    "### Part II. Emotion Classification (5 pts)\n",
    "\n",
    "Use the `datasets/emotions_dataset.zip` (see the original Dataset source on [Kaggle](https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp)) to build a classification model that predicts the emotion of sentence. If you would like, you may classify only the top 4 emotions, and group all other classes as `Other`. \n",
    "\n",
    "In order to earn full credit, you must:\n",
    "\n",
    "* Show the performance of your model with `CountVectorizer`, `TfIdfVectorizer`, `word2vec`, and `glove` embeddings.\n",
    "    - for `word2vec`, make sure not to use the `en_core_web_sm` dataset (these are not real embeddings)\n",
    "* Perform text preprocessing (or explain why it was not necessary):\n",
    "    - stopword removal\n",
    "    - ngram tokenization\n",
    "    - stemming/lemmatization\n",
    "    - fuzzy matching / regex cleaning / etc. (as you deem necessary, but show that you analyzed the text to make your decision)\n",
    "* Show **AUROC / F1 scores** for on the holdout (test + validation) datasets.\n",
    "* A brief discussion (2-3 sentences) of what could improve your model and why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8f323",
   "metadata": {},
   "source": [
    "### 1. Importing Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d677325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0935c085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zz/gt_tn3454131nr1dxcqk06b40000gn/T/ipykernel_81248/1303294747.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = train_df.append(test_df.append(val_df , ignore_index=True) , ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion   type\n",
       "0                            i didnt feel humiliated  sadness  train\n",
       "1  i can go from feeling so hopeless to so damned...  sadness  train\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger  train\n",
       "3  i am ever feeling nostalgic about the fireplac...     love  train\n",
       "4                               i am feeling grouchy    anger  train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../datasets/emotions/train.txt',header=None, names=['text'])\n",
    "train_df[['text','emotion']] = train_df['text'].str.split(';',expand=True)\n",
    "train_df['type'] = \"train\"\n",
    "test_df = pd.read_csv('../datasets/emotions/test.txt',header=None, names=['text'])\n",
    "test_df[['text','emotion']] = test_df['text'].str.split(';',expand=True)\n",
    "test_df['type'] = \"test\"\n",
    "val_df = pd.read_csv('../datasets/emotions/val.txt',header=None, names=['text'])\n",
    "val_df[['text','emotion']] = val_df['text'].str.split(';',expand=True)\n",
    "val_df['type'] = \"val\"\n",
    "\n",
    "df = train_df.append(test_df.append(val_df , ignore_index=True) , ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad8a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(20000, 3)\n",
      "['train' 'test' 'val']\n"
     ]
    }
   ],
   "source": [
    "# Checking if all the data have been appended correctly\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(val_df.shape)\n",
    "print(df.shape)\n",
    "\n",
    "print(df['type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346525d6",
   "metadata": {},
   "source": [
    "### 2. Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c90535",
   "metadata": {},
   "source": [
    "- **Stopword removal:** We want remove stopwords so that we can only focus on words that are relavent for predicting the sentiment. We don't want common recurring words to override the actual sentiment in the text. However, we do want to remove certain words from the list of stopwords from the packages we are using because in our text, words that change the meaning to the opposute like didn't, haven't, or wasn't. \n",
    "- **N-gram tokenization:** We want to do n-gram tokenization because there are word pairings that change the sentiment. For example \"didn't feel humiliated\" mean something different compared to just \"humiliated\". So we want to account for bigrams and trigrams. \n",
    "- **Stemming/lemmetization:** we don't want to do stemming or lemmetization because there are certain words that may have lose its original sentiment if we lemmetize them. For example, hope and hopeless would both be lemmetized to hope, but this completely alters the sentiment of the \"hopeless\". \n",
    "- **Fuzzy-matching/Regex:** Upon drawing multiple randome samples of the text and physically examining the text in the dataset, we determined that there aren't too many spelling errors or issues with text quality. Therefore, we won't be performing fuzzy-matching or Regex. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b61fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a317c68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>type</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>didnt humiliated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>go hopeless damned hopeful around someone care...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "      <td>im grabbing minute post greedy wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>train</td>\n",
       "      <td>ever nostalgic fireplace know still property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "      <td>grouchy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ive been feeling a little burdened lately wasn...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>ive little burdened lately wasnt sure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ive been taking or milligrams or times recomme...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>train</td>\n",
       "      <td>ive taking milligrams times recommended amount...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i feel as confused about life as a teenager or...</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>confused life teenager jaded year old man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i have been with petronas for years i feel tha...</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>petronas years petronas performed well made hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i feel romantic too</td>\n",
       "      <td>love</td>\n",
       "      <td>train</td>\n",
       "      <td>romantic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   emotion   type  \\\n",
       "0                            i didnt feel humiliated   sadness  train   \n",
       "1  i can go from feeling so hopeless to so damned...   sadness  train   \n",
       "2   im grabbing a minute to post i feel greedy wrong     anger  train   \n",
       "3  i am ever feeling nostalgic about the fireplac...      love  train   \n",
       "4                               i am feeling grouchy     anger  train   \n",
       "5  ive been feeling a little burdened lately wasn...   sadness  train   \n",
       "6  ive been taking or milligrams or times recomme...  surprise  train   \n",
       "7  i feel as confused about life as a teenager or...      fear  train   \n",
       "8  i have been with petronas for years i feel tha...       joy  train   \n",
       "9                                i feel romantic too      love  train   \n",
       "\n",
       "                                          text_clean  \n",
       "0                                   didnt humiliated  \n",
       "1  go hopeless damned hopeful around someone care...  \n",
       "2               im grabbing minute post greedy wrong  \n",
       "3       ever nostalgic fireplace know still property  \n",
       "4                                            grouchy  \n",
       "5              ive little burdened lately wasnt sure  \n",
       "6  ive taking milligrams times recommended amount...  \n",
       "7          confused life teenager jaded year old man  \n",
       "8  petronas years petronas performed well made hu...  \n",
       "9                                           romantic  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add to stopword: feel, feeling\n",
    "stop = stopwords.words('english')\n",
    "stop.extend(['feel','feeling'])\n",
    "\n",
    "# Remove from stopword\n",
    "stop.remove('didn')\n",
    "stop.remove('didn\\'t')\n",
    "\n",
    "# https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "df['text_clean'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf384d",
   "metadata": {},
   "source": [
    "#### Word2Vec Build Classification Model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f98e114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jewonju/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py:1895: UserWarning: [W123] Argument disable with value ['ner', 'parser'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# load the language model, but we disable the ner (named entity recognition) and parser (dependency parser)\n",
    "# since we don't need them for our use case to speed things up\n",
    "nlp = spacy.load('en_core_web_md', disable = ['ner', 'parser'])\n",
    "\n",
    "import numpy as np\n",
    "def process_text(text):\n",
    "  \"\"\"\n",
    "  This function will use Spacy to perform stopword removal and lemmatization.\n",
    "  \"\"\"\n",
    "  doc = nlp(text)\n",
    "  processed_text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "  # this will get the word2vec embeddings for the processed text (the average of each token in the doc's word2vec embeddings)\n",
    "  return np.array(nlp(processed_text).vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e31adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas' apply(...) method to apply this process_text function to each row's text field\n",
    "df[\"vectors\"] = df.text.apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d336d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([vector for vector in df[\"vectors\"]])\n",
    "y = df[\"emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70cc7c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f53d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset is (15000, 300)\n",
      "Training target is (15000,)\n",
      "Test dataset is (5000, 300)\n",
      "Test target is (5000,)\n",
      "0.6534\n",
      "0.6272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jewonju/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "print(f\"Training dataset is {X_train.shape}\")\n",
    "print(f\"Training target is {y_train.shape}\")\n",
    "print(f\"Test dataset is {X_test.shape}\")\n",
    "print(f\"Test target is {y_test.shape}\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "training_predictions = logistic_regression.predict(X_train)\n",
    "training_predictions[:5] # these are our model's prediction for first 5 documents in the training dataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "confusion_matrix(y_train, training_predictions)\n",
    "\n",
    "# we get 65.3% accuracy on the training data using word2vec \n",
    "print(logistic_regression.score(X_train, y_train))\n",
    "\n",
    "# we got 62.7% accuracy on the test data\n",
    "print(logistic_regression.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36e08c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8674393049275498"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_probabilities = logistic_regression.predict_proba(X_test)\n",
    "roc_auc_score(y_test, y_probabilities, multi_class=\"ovo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4715ec",
   "metadata": {},
   "source": [
    "#### Count Vectorizer Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc0fb677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jewonju/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.89525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 513,   13,   22,    4,   31,    0],\n",
       "       [  21,  361,   12,    5,   21,   20],\n",
       "       [   8,    7, 1219,   46,   19,   11],\n",
       "       [   3,    1,   55,  267,    4,    1],\n",
       "       [  34,   13,   27,    3, 1098,    0],\n",
       "       [   0,   22,    9,    0,    7,  123]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"text_clean\"])\n",
    "y = df[\"emotion\"].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.2)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "print(f\"Training accuracy: {np.mean(y_pred == y_test)}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55acfdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997672478362439"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_probabilities = lr.predict_proba(X_test)\n",
    "\n",
    "roc_auc_score(y_pred, y_probabilities, multi_class=\"ovo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b6db9",
   "metadata": {},
   "source": [
    "#### TFIDF Vectorizer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38352398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jewonju/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.86825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 464,    7,   39,    1,   56,    0],\n",
       "       [  25,  338,   44,    2,   28,    4],\n",
       "       [   5,    2, 1304,   18,   21,    3],\n",
       "       [   3,    1,  105,  210,   13,    0],\n",
       "       [  20,    9,   39,    1, 1092,    1],\n",
       "       [   3,   28,   31,    1,   17,   65]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"text_clean\"])\n",
    "y = df[\"emotion\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.2)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "print(f\"Training accuracy: {np.mean(y_pred == y_test)}\")\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afa888e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979881006871151"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_probabilities = lr.predict_proba(X_test)\n",
    "\n",
    "roc_auc_score(y_pred, y_probabilities, multi_class=\"ovo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed343a87219b975df84d5260125e17249ec3c1727fa796fd58ecd90702b8aed1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
